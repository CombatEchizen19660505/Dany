import requests
import pandas as pd
from datetime import datetime, timedelta
from dateutil import parser
from IPython.display import display
import math
import json
import numpy as np
import time  # time モジュールをインポート（sleep用）
from datetime import datetime, timedelta
from dateutil import parser

from datetime import time as datetime_time

# Excelファイルからアーカイブシートとスパムシートを読み込む
excel_archive_path = r"C:\Users\valen\OneDrive\デスクトップ\Solanaトークン_API抽出_本番環境.xlsx"
excel_archive_path_spam = r"C:\Users\valen\OneDrive\デスクトップ\df_spam.xlsx"
df_Archive = pd.read_excel(excel_archive_path, sheet_name='アーカイブ')
df_Spam = pd.read_excel(excel_archive_path_spam, sheet_name='スパム')

def get_new_pools(network="solana", start_page=2):
    all_pool_data = []
    page = start_page
    max_age_found = 0
    current_datetime = datetime.now().strftime('%Y/%m/%d %H:%M')
    archive_addresses = set(df_Archive['プールアドレス'].dropna())  # アーカイブのプールアドレスをセットとして保持
    
    while max_age_found < 15:
        url = f"https://api.geckoterminal.com/api/v2/networks/solana/new_pools"
        
        params = {
            "include": "base_token,quote_token", 
            "page": page
        }
        
        try:
            response = requests.get(url, params=params)
            response.raise_for_status()
            data = response.json()
            
            if not data["data"]:
                break
                
            for pool in data["data"]:
                pool_info = pool["attributes"]
                pool_address = pool_info['address']
                
                # プールアドレスがアーカイブに存在する場合、ループを抜ける
                if pool_address in archive_addresses:
                    print(f"既存のプールアドレスが見つかりました: {pool_address}")
                    return pd.DataFrame(all_pool_data)
                
                created_at = parser.parse(pool_info['pool_created_at'])
                current_time = datetime.now(created_at.tzinfo)
                age = current_time - created_at
                age_minutes = age.total_seconds() / 60
                
                max_age_found = max(max_age_found, age_minutes)
                
                price_changes = pool_info['price_change_percentage'] or {}
                transactions = pool_info['transactions'] or {}
                volume_data = pool_info['volume_usd'] if pool_info['volume_usd'] else {}
                
                pool_dict = {
                    '日時': current_datetime,
                    '名前': pool_info['name'],
                    'Age[min]': f"{age_minutes:.1f}",
                    '価格[$]': f"{float(pool_info['base_token_price_usd']):.6f}" if pool_info['base_token_price_usd'] else None,
                    'LP[万$]': math.ceil(float(pool_info['reserve_in_usd'])/10000) if pool_info['reserve_in_usd'] else None,
                    'FDV[万$]': math.ceil(float(pool_info['fdv_usd'])/10000) if pool_info['fdv_usd'] else None,
                    'プールアドレス': pool_address,
                    '5m取引量[$]': math.floor(float(volume_data.get('m5', 0))),
                    '1H取引量[$]': math.floor(float(volume_data.get('h1', 0))),
                    '6H取引量[$]': math.floor(float(volume_data.get('h6', 0))),


                    '5m[%]': price_changes.get('m5'),
                    '1H[%]': price_changes.get('h1'),
                    '6H[%]': price_changes.get('h6'),
                    '24H[%]': price_changes.get('h24'),
                    
                    '5m buys': transactions.get('m5', {}).get('buys'),
                    '15m buys': transactions.get('m15', {}).get('buys'),
                    '30m buys': transactions.get('m30', {}).get('buys'),
                    '1h buys': transactions.get('h1', {}).get('buys'),

                    '5m sells': transactions.get('m5', {}).get('sells'),
                    '15m sells': transactions.get('m15', {}).get('sells'),
                    '30m sells': transactions.get('m30', {}).get('sells'),
                    '1h sells': transactions.get('h1', {}).get('sells'),

                    '5m buyers': transactions.get('m5', {}).get('buyers'),
                    '15m buyers': transactions.get('m15', {}).get('buyers'),
                    '30m buyers': transactions.get('m30', {}).get('buyers'),
                    '1h buyers': transactions.get('h1', {}).get('buyers'),

                    '5m sellers': transactions.get('m5', {}).get('sellers'),
                    '15m sellers': transactions.get('m15', {}).get('sellers'),
                    '30m sellers': transactions.get('m30', {}).get('sellers'),
                    '1h sellers': transactions.get('h1', {}).get('sellers'),

                    '24H取引量[$]': math.floor(float(volume_data.get('h24', 0)))
                }
                
                all_pool_data.append(pool_dict)
            
            print(f"ページ {page} を処理中... 最大作成時間: {max_age_found:.1f}分")
            page += 1
            
        except requests.exceptions.RequestException as e:
            print(f"エラー: {e}")
            break
    
    df_Buffer = pd.DataFrame(all_pool_data)
    df_Buffer['名前'] = df_Buffer['名前'].str.replace(' / SOL', '')
    
    # スパムリストとの完全一致チェック
    spam_exact_matches = df_Spam['完全一致'].dropna().str.lower()
    df_Buffer = df_Buffer[~df_Buffer['名前'].str.lower().isin(spam_exact_matches)]
    
    # スパムリストとの部分一致チェック
    spam_partial_matches = df_Spam['部分一致'].dropna().str.lower()
    df_Buffer = df_Buffer[~df_Buffer['名前'].str.lower().apply(
        lambda x: any(spam_str in x for spam_str in spam_partial_matches)
    )]
    
    df_Buffer['Age[min]'] = df_Buffer['Age[min]'].astype(float)
    
    price_cols = ['5m[%]', '1H[%]', '6H[%]', '24H[%]']
    for col in price_cols:
        df_Buffer[col] = pd.to_numeric(df_Buffer[col], errors='coerce')
    
    df_Buffer = df_Buffer[df_Buffer['Age[min]'] > 5]
    df_Buffer = df_Buffer[df_Buffer['LP[万$]'] > 3]
    df_Buffer = df_Buffer[df_Buffer['FDV[万$]'] >= 3]
    # df_Buffer = df_Buffer[~(df_Buffer[price_cols] < -50).any(axis=1)]
    df_Buffer = df_Buffer[df_Buffer['5m[%]'] != 0]
    
    pd.set_option('display.max_columns', None)
    pd.set_option('display.max_rows', None)
    pd.set_option('display.width', None)
    pd.set_option('display.max_colwidth', None)
    
    df_Buffer = df_Buffer[df_Buffer['5m[%]'] != 0]
    df_Buffer['Age[min]'] = pd.to_numeric(df_Buffer['Age[min]'])
    df_Buffer = df_Buffer.sort_values('Age[min]', ascending=True)
    df_Buffer = df_Buffer.drop_duplicates(subset=['プールアドレス'], keep='last')
    
    # 取引量が100以下の行を除外
    volume_cols = ['5m取引量[$]', '1H取引量[$]', '6H取引量[$]', '24H取引量[$]']
    df_Buffer = df_Buffer[df_Buffer[volume_cols].gt(100).any(axis=1)]
    
    # # バイヤー数が10以下の行を除外
    buyers_cols = ['5m buyers', '15m buyers', '30m buyers', '1h buyers']
    df_Buffer = df_Buffer[df_Buffer[buyers_cols].gt(10).any(axis=1)]
    


    # セラー数が0の行を除外
    sellers_cols = ['5m sellers', '15m sellers', '30m sellers', '1h sellers']
    df_Buffer = df_Buffer[~df_Buffer[sellers_cols].eq(0).any(axis=1)]
    
    # 価格変化率が5000%以上の行を削除
    df_Buffer = df_Buffer[~(df_Buffer[['5m[%]', '1H[%]'] ].ge(5000).any(axis=1))]
    
    # 価格変化率を小数点1桁に丸める
    df_Buffer[['5m[%]', '1H[%]']] = df_Buffer[['5m[%]', '1H[%]']].round(1)
    
    # 特定の列を削除
    columns_to_drop = ['6H[%]', '24H[%]', '5m buys', '15m buys', '30m buys', '1h buys', 
                      '5m sells', '15m sells', '30m sells', '1h sells',
                      '6H取引量[$]', '24H取引量[$]']
    df_Buffer = df_Buffer.drop(columns=columns_to_drop)
   
       
    # 価格変化率を小数点1桁に丸める
    df_Buffer[['5m[%]', '1H[%]']] = df_Buffer[['5m[%]', '1H[%]']].round(1)
    
    # 5m buyers が 5m sellers より小さい行を削除
    df_Buffer = df_Buffer[df_Buffer['5m buyers'] >= df_Buffer['5m sellers']]

    # 存在する列のみを削除
    columns_to_drop = [col for col in ['6H[%]', '24H[%]', '5m buys', '15m buys', '30m buys', '1h buys', 
                                     '5m sells', '15m sells', '30m sells', '1h sells',
                                     '6H取引量[$]', '24H取引量[$]'] if col in df_Buffer.columns]
    
    
    if columns_to_drop:
        df_Buffer = df_Buffer.drop(columns=columns_to_drop)

    # OHLCVデータの取得と連結
    base_url = "https://api.geckoterminal.com/api/v2"
    timeframe = "minute"
    aggregate = "5"

    current_time = datetime.now()
    success_count = 0
    failure_count = 0
    failure_reasons = []

    # 各プールアドレスに対してOHLCVデータを取得
    ohlcv_data_list = []

    # プールアドレスを一行ずつ処理
    for index, row in df_Buffer.iterrows():
        pool_address = row['プールアドレス']
        
        # Age[min]の値を小数点以下切り捨てて取得
        try:
            age_minutes = math.floor(float(row['Age[min]']))
            
            # 年齢が一定値を超える場合はスキップ
            if age_minutes > 1440:  # 24時間を超える場合
                print(f"警告: プールの年齢が24時間を超えています: {age_minutes}分")
                failure_reasons.append(f"Age too old: {age_minutes} minutes")
                failure_count += 1
                continue
                
            # 各プールの年齢に基づいてタイムスタンプを計算
            twenty_mins_ago = current_time - timedelta(minutes=age_minutes)
            # パターン1 Age[min]から逆算し時間を指定する。
            # before_timestamp = int(twenty_mins_ago.timestamp())
            
            # パターン２ プールの年齢に関係なく、直近のデータを取得する場合⇒これが正解。
            before_timestamp = int(current_time.timestamp())

            # パターン３ 一定期間（例：30分）前までのデータを取得する場合
            # before_timestamp = int((current_time - timedelta(minutes=30)).timestamp())

            current_timestamp = int(datetime.now().timestamp())
            if before_timestamp > current_timestamp:
                before_timestamp = current_timestamp
            
            endpoint = f"{base_url}/networks/solana/pools/{pool_address}/ohlcv/{timeframe}"
            params = {
                "aggregate": aggregate,
                "before_timestamp": before_timestamp,
                "limit": "100",
                "currency": "usd",
                "token": "base",
                "include": "base_token,quote_token"  # このパラメータを追加
            }
            
            response = requests.get(endpoint, params=params)
            response.raise_for_status()
            data = response.json()

            if "data" in data and "attributes" in data["data"] and "ohlcv_list" in data["data"]["attributes"]:
                ohlcv_list = data["data"]["attributes"]["ohlcv_list"]

                # base_tokenのアドレスを取得
                # base_token_address = None
                # if "included" in data:
                #     for included_item in data["included"]:
                #         if included_item["type"] == "token" and included_item.get("attributes", {}).get("role") == "base":
                #             base_token_address = included_item["attributes"].get("address")
                #             break

                # タイムスタンプ（最初の要素）でソート
                sorted_ohlcv = sorted(ohlcv_list, key=lambda x: x[0])
                base_token_address= data['meta']['base']['address']


                # if len(ohlcv_list) >= 2:
                #     ohlcv_data = {
                #         'プールアドレス': pool_address,
                #         'コントラクトアドレス': base_token_address,  # base_tokenのアドレスを追加
                #         '1_Ti': datetime.fromtimestamp(ohlcv_list[1][0]).strftime('%Y/%m/%d %H:%M'),
                #         '1-Op': np.ceil(ohlcv_list[1][1] * 10000000) / 10000000,
                #         '1-Hi': np.ceil(ohlcv_list[1][2] * 10000000) / 10000000,
                #         '1-Lo': np.ceil(ohlcv_list[1][3] * 10000000) / 10000000,
                #         '1-Cl': np.ceil(ohlcv_list[1][4] * 10000000) / 10000000,
                #         '1-Vo': np.floor(ohlcv_list[1][5]),
                #         '2_Ti': datetime.fromtimestamp(ohlcv_list[0][0]).strftime('%Y/%m/%d %H:%M'),
                #         '2-Op': np.ceil(ohlcv_list[0][1] * 10000000) / 10000000,
                #         '2-Hi': np.ceil(ohlcv_list[0][2] * 10000000) / 10000000,
                #         '2-Lo': np.ceil(ohlcv_list[0][3] * 10000000) / 10000000,
                #         '2-Cl': np.ceil(ohlcv_list[0][4] * 10000000) / 10000000,
                #         '2-Vo': np.floor(ohlcv_list[0][5])
                #     }

                if len(sorted_ohlcv) >= 2:
                    # print(sorted_ohlcv)
                    ohlcv_data = {
                        'プールアドレス': pool_address,
                        'コントラクトアドレス': base_token_address,
                        # 最も古いデータ（[0]）を1_Ti, 1-Op等に割り当て
                        '1_Ti': datetime.fromtimestamp(sorted_ohlcv[0][0]).strftime('%Y/%m/%d %H:%M'),
                        '1-Op': np.ceil(sorted_ohlcv[0][1] * 10000000) / 10000000,
                        '1-Hi': np.ceil(sorted_ohlcv[0][2] * 10000000) / 10000000,
                        '1-Lo': np.ceil(sorted_ohlcv[0][3] * 10000000) / 10000000,
                        '1-Cl': np.ceil(sorted_ohlcv[0][4] * 10000000) / 10000000,
                        '1-Vo': np.floor(sorted_ohlcv[0][5]),
                        # 2番目に古いデータ（[1]）を2_Ti, 2-Op等に割り当て
                        '2_Ti': datetime.fromtimestamp(sorted_ohlcv[1][0]).strftime('%Y/%m/%d %H:%M'),
                        '2-Op': np.ceil(sorted_ohlcv[1][1] * 10000000) / 10000000,
                        '2-Hi': np.ceil(sorted_ohlcv[1][2] * 10000000) / 10000000,
                        '2-Lo': np.ceil(sorted_ohlcv[1][3] * 10000000) / 10000000,
                        '2-Cl': np.ceil(sorted_ohlcv[1][4] * 10000000) / 10000000,
                        '2-Vo': np.floor(sorted_ohlcv[1][5])
                    }

                    ohlcv_data_list.append(ohlcv_data)
                    success_count += 1
                else:
                    failure_reasons.append(f"Insufficient data points: {len(ohlcv_list)}")
                    failure_count += 1
            else:
                failure_reasons.append("Invalid response structure")
                failure_count += 1
                
        except requests.exceptions.RequestException as e:
            print(f"エラー: {pool_address} の OHLCV データ取得に失敗しました - {str(e)}")
            failure_reasons.append(f"Request error: {str(e)}")
            failure_count += 1
            continue
        except Exception as e:
            print(f"予期せぬエラー: {str(e)}")
            failure_reasons.append(f"Unexpected error: {str(e)}")
            failure_count += 1
            continue
        
        # APIレート制限を考慮して待機時間を増やす
        time.sleep(0.1)  # 0.5秒に増やす

    # OHLCVデータをDataFrameに変換
    if ohlcv_data_list:
        df_ohlcv = pd.DataFrame(ohlcv_data_list)
        # プールアドレスでdf_BufferとOHLCVデータを結合
        df_Buffer = pd.merge(df_Buffer, df_ohlcv, on='プールアドレス', how='left')

    else:
        print("\n=== WARNING: No OHLCV data was collected ===")

    # 取引量が100000未満の行を削除（OHLCVデータ結合後に実行）
    df_Buffer = df_Buffer[
        (df_Buffer['1-Vo'] >= 100000) | 
        (df_Buffer['2-Vo'] >= 100000)
    ]

    # 1-Op < 1-Cl かつ 2-Op > 2-Cl の条件でフィルタリング
    df_Buffer = df_Buffer[
        (df_Buffer['1-Op'] > df_Buffer['1-Cl']) & 
        (df_Buffer['2-Op'] < df_Buffer['2-Cl'])
    ]

    # URL列を追加
    df_Buffer['URL'] = 'https://www.geckoterminal.com/ja/solana/pools/' + df_Buffer['プールアドレス']

    # 指定された列を削除
    columns_to_remove = ['30m buyers', '1h buyers', '30m sellers', '1h sellers']
    df_Buffer = df_Buffer.drop(columns=columns_to_remove, errors='ignore')

    # excel_path = r"C:\Users\valen\OneDrive\デスクトップ\Solanaトークン_API抽出-デバッグ.xlsx"
    excel_path = r"C:\Users\valen\OneDrive\デスクトップ\Solanaトークン_API抽出_本番環境.xlsx"

    # LINE通知用の設定
    TOKEN = "U5wJYEhcZvdLMuWYNYW7hluUCuOJY74p5LqRhFbuuLi"
    api_url = "https://notify-api.line.me/api/notify"
    headers = {'Authorization': f'Bearer {TOKEN}'}

    try:
        # 既存のExcelファイルを読み込む
        existing_df = pd.read_excel(excel_path, sheet_name='アーカイブ')

        # 現在の日本時間を取得
        current_time = datetime.now().time()
        notification_start = datetime_time(5, 30)  # 5:30
        notification_end = datetime_time(22, 0)    # 22:00

        # 新しいデータがあり、かつ通知時間内の場合にLINE通知を送信
        if not df_Buffer.empty and notification_start <= current_time <= notification_end:
            for _, row in df_Buffer.iterrows():
                message = (
                    f"\n名前: {row['名前']}"
                    f"\nコントラクトアドレス: {row['コントラクトアドレス']}"
                    f"\nURL: {row['URL']}"
                )
                payload = {'message': message}
                requests.post(api_url, headers=headers, data=payload)
                time.sleep(0.1)  # LINE APIの制限を考慮して1秒待機
        
        # 新しいデータを既存のデータに追加
        combined_df = pd.concat([existing_df, df_Buffer], ignore_index=True)


    except FileNotFoundError:
        # ファイルが存在しない場合は新規作成
        combined_df = df_Buffer

    # Excelライターを作成（既存ファイルを上書きモード）
    with pd.ExcelWriter(excel_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:
        # 結合したDataFrameをアーカイブシートに出力
        combined_df.to_excel(writer, sheet_name='アーカイブ', index=False)
        
        # ワークシートを取得
        worksheet = writer.sheets['アーカイブ']
        
        # 列幅の自動調整
        for column in worksheet.columns:
            max_length = 0
            column = [cell for cell in column]
            for cell in column:
                try:
                    if len(str(cell.value)) > max_length:
                        max_length = len(str(cell.value))
                except:
                    pass
            adjusted_width = (max_length + 2)
            worksheet.column_dimensions[column[0].column_letter].width = adjusted_width
        
        # URL列にハイパーリンクを設定
        url_col_index = combined_df.columns.get_loc('URL') + 1  # Excelは1から始まるため+1
        for row in range(2, len(combined_df) + 2):  # ヘッダー行を除くため2から開始
            cell = worksheet.cell(row=row, column=url_col_index)
            cell.hyperlink = cell.value
            cell.style = 'Hyperlink'

    return df_Buffer

if __name__ == "__main__":
    df_Buffer = get_new_pools()
    print("動作完了")
