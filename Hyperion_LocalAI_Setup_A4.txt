ローカル生成AI（Hyperion / Windows 11 Pro）環境構築まとめ（A4 1〜2枚想定）
======================================================

前提（要件）
- 期限：4月末までに運用開始（担当1名のためスピード最優先）
- OS：Windowsのみ（WSL2なし運用を優先）
- LLM：gpt-oss 20B（GPU/CUDAで高速化したい）
- 目的：Hyperionを「LLM＋RAG（社内資料検索）」のサーバーとしてまとめて運用

Hyperion スペック（記憶）
- Windows 11 Pro / Ryzen 9 9950X3D / RTX 5090 32GB / DDR5 128GB / 2TB Gen4 SSD


1) LLMを“サーバー化”する（推論エンジン／API）
--------------------------------------------
A. Microsoft Foundry Local（最短で動かす本命）
- 目的：Windowsでgpt-oss 20Bを素早く起動・検証する
- 特徴：導入が軽い。CLI/SDK/REST連携の導線が太い
- 使いどころ：最初に「モデルがGPUで安定稼働する」状態を最速で作る

B. LM Studio（OpenAI互換APIで自作連携が速い）
- 目的：既存のOpenAI SDK（Python/JS/C#）を流用して最短で接続する
- 特徴：GUIでモデル管理しやすい／ローカルでOpenAI互換APIを立てやすい
- 注意：社内利用ポリシーや利用規約は最終的に社内規程に合わせて確認

C. Ollama（運用を単純化しやすい）
- 目的：ローカルLLM運用を簡素にしたい
- 特徴：サービス運用の考え方と相性がよい（構成がシンプルになりがち）


2) RAG（社内資料検索）の“アプリ側”を最短で用意する
--------------------------------------------
AnythingLLM Desktop（Difyを避けたい場合の現実的な最短ルート）
- 目的：UI込みで「社内資料RAGチャット」を最速で形にする
- 特徴：ローカルRAGの完成品に近い。導入→資料投入→回答までが速い
- 注意：Desktopの利用条件（社内利用可否）と、社内規程（情報持ち出し禁止等）を合わせて確認


3) ベクトルDB（RAGの検索基盤）— Windowsで堅くいく選択肢
--------------------------------------------
方針：最初は「軽い・移行しやすい」もの → 同時接続や規模が増えたらサーバー型へ

A. LanceDB（組み込み型 / 軽い）
- 目的：まず動くRAGを速く作る（アプリに組み込みやすい）

B. pgvector（PostgreSQL統合）
- 目的：既存DB運用（権限・監査・バックアップ）と統合して運用を楽にする
- 向く：社内でPostgreSQL運用が既にある／DBで一元管理したい

C. Qdrant（サーバー型 / スケール向き）
- 目的：将来的にユーザー数やデータ量が増えた際の拡張
- 注意：Docker/WSL2を使わない場合、ビルドや配布方法の検討が必要になりやすい


4) GPU/CUDAで速度を出すための土台（最低限）
--------------------------------------------
- NVIDIA Driver（最新版）
- CUDA Toolkit（必要に応じて。導入範囲は社内規程と整合）
- Foundry Local / LM Studio / Ollama いずれかで「GPU利用が有効」になっていることを確認
  - まずは簡単なプロンプトで生成速度（token/s）を測定して基準値を作る


5) サーバー運用の小物（地味に効く）
--------------------------------------------
- Windowsサービス化：WinSW など（自動起動・再起動での復旧を簡単に）
- リバースプロキシ：Caddy または nginx（社内TLS・経路制御・認証の入口に）
- ログ：標準出力のログ回収／ローテーション（障害調査が速くなる）


推奨構成（スピード最優先の“勝ち筋”）
--------------------------------------------
最短の型（まず動かして、後から強化できる順番）
1) Foundry Local で gpt-oss 20B をGPUで安定稼働させる（LLM単体の稼働確立）
2) AnythingLLM Desktop で「RAGとして使える形」を先に通す（業務導線の確立）
3) ベクトルDBは最初は LanceDB か pgvector（簡単・移行しやすい）
   - 同時接続や規模が増えたら Qdrant に移行検討
4) WinSW＋Caddy/nginx でサービス化（運用の形にする）

この順序の狙い
- “モデルが動く” → “業務で使える形” → “後から拡張” の一本道にして、締切に勝つ


補足（ライセンス観点の最低限の整理）
--------------------------------------------
- gpt-oss 20B：Apache 2.0（商用利用可）
- アプリ／DB／周辺ツール：無料で商用利用可能なものを優先
- ただし、最終判断は「各ツールの利用規約」と「社内規程（情報管理・持ち出し・監査）」に合わせて実施

（以上）
